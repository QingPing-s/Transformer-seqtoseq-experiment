

# Transformer 实现作业 - 进阶功能

### 1. 梯度裁剪与监控
- 实现了梯度裁剪防止梯度爆炸
- 梯度分布可视化分析
- 训练稳定性监控

### 2. 困惑度分析
- 自动计算训练和验证困惑度
- 困惑度曲线可视化
- 模型性能量化评估

### 3. 消融实验系统
支持以下消融实验配置：
- `base`: 基础配置
- `no_positional_encoding`: 无位置编码
- `no_residual`: 无残差连接和层归一化  
- `single_head`: 单头注意力机制
- `small_model`: 小模型配置
- `no_scheduler`: 无学习率调度
- `high_dropout`: 高dropout率

### 4. 实验管理
- 自动保存实验配置和结果
- 训练过程可视化
- 实验结果对比分析

  🏗️ 模型架构
Transformer 架构
本项目实现了标准的 Transformer Encoder-Decoder 架构，包含以下核心组件：

多头注意力机制：支持自注意力和交叉注意力
前馈神经网络：位置前馈网络（Position-wise Feed-Forward Networks）
残差连接与层归一化：增强梯度流动和训练稳定性
相对位置偏置：T5 风格的相对位置编码，增强模型对序列位置信息的感知
相对位置偏置
采用 T5 风格的相对位置偏置实现：

创建大小为 (2*max_distance+1, num_heads) 的偏置表
对于位置 i 和 j 之间的注意力分数添加偏置 b_{i-j}
通过学习的方式获取相对位置信息，相比绝对位置编码更具表达能力
